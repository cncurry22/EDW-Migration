Data Import
import domojupyter as domo
df = domo.read_dataframe('VwRateAnalyticsToolUnclustered', query='SELECT * FROM table')
# Display the first few rows and summary information of the dataframe
df.head(), df.info()

# Preprocess the data
df = df.sort_values(by=['date', 'communityname', 'unittype'], ascending=[False, True, True])
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 278955 entries, 0 to 278954
Data columns (total 69 columns):
 #   Column                                Non-Null Count   Dtype         
---  ------                                --------------   -----         
 0   date                                  278955 non-null  datetime64[ns]
 1   academicyearbegindate                 278955 non-null  datetime64[ns]
 2   academicyearenddate                   278955 non-null  datetime64[ns]
 3   weekbegindate                         278955 non-null  datetime64[ns]
 4   weekenddate                           278955 non-null  datetime64[ns]
 5   keystonereportingind                  278955 non-null  object        
 6   propertytype                          278955 non-null  object        
 7   totalcommunitybeds                    278955 non-null  Int64         
 8   college                               278955 non-null  object        
 9   proximitytocampus                     278955 non-null  float64       
 10  vintage                               278955 non-null  Int64         
 11  cgmgmtstatus                          278955 non-null  object        
 12  propertycd                            278955 non-null  object        
 13  communityname                         278955 non-null  object        
 14  unittype                              278955 non-null  object        
 15  newleasesustainablecap                269228 non-null  float64       
 16  renewalleasesustainablecap            210247 non-null  float64       
 17  totalmarketunitbeds                   250293 non-null  Int64         
 18  avgmarketunitsqft                     248335 non-null  Int64         
 19  avgmarketunitrate                     250272 non-null  float64       
 20  marketunitprelease                    250286 non-null  float64       
 21  marketunitoccupancy                   250006 non-null  float64       
 22  ytdtotalleases                        278955 non-null  float64       
 23  ytdnewleases                          278955 non-null  float64       
 24  ytdrenewalleases                      278955 non-null  float64       
 25  signedmonthlyrentusd                  278955 non-null  float64       
 26  newleasesignedmonthlyrentusd          278955 non-null  float64       
 27  renewalleasesignedmonthlyrentusd      278955 non-null  float64       
 28  totalweeklyleases                     274301 non-null  float64       
 29  newweeklyleases                       274301 non-null  float64       
 30  totalweeklysignedrent                 274301 non-null  float64       
 31  newweeklysignedrent                   274301 non-null  float64       
 32  totaltrailing30dayleases              259398 non-null  float64       
 33  newtrailing30dayleases                259398 non-null  float64       
 34  totaltrailing30daysignedrent          259398 non-null  float64       
 35  newtrailing30daysignedrent            259398 non-null  float64       
 36  totaltrailing60dayleases              241411 non-null  float64       
 37  newtrailing60dayleases                241411 non-null  float64       
 38  totaltrailing60daysignedrent          241411 non-null  float64       
 39  newtrailing60daysignedrent            241411 non-null  float64       
 40  totalyearlyleasevariance              90188 non-null   float64       
 41  newyearlyleasevariance                90188 non-null   float64       
 42  totalyearlysignedrentvariance         90188 non-null   float64       
 43  newyearlysignedrentvariance           90188 non-null   float64       
 44  weeklyaskingratechange                219056 non-null  float64       
 45  currentaskingrate                     222136 non-null  float64       
 46  dailyleads                            272243 non-null  Int64         
 47  trailing7dayleads                     272243 non-null  Int64         
 48  trailing4weekaverageleads             272243 non-null  Int64         
 49  trailingadditional4weekaverageleads   264355 non-null  Int64         
 50  prioryeartrailing7dayleads            146805 non-null  Int64         
 51  dailytours                            272243 non-null  Int64         
 52  trailing7daytours                     272243 non-null  Int64         
 53  trailing4weekaveragetours             272243 non-null  Int64         
 54  trailingadditional4weekaveragetours   264355 non-null  Int64         
 55  prioryeartrailing7daytours            146805 non-null  Int64         
 56  dailyapps                             272243 non-null  Int64         
 57  trailing7dayapps                      272243 non-null  Int64         
 58  trailing4weekaverageapps              272243 non-null  Int64         
 59  trailingadditional4weekaverageapps    264355 non-null  Int64         
 60  prioryeartrailing7dayapps             146805 non-null  Int64         
 61  dailyleases                           272243 non-null  Int64         
 62  trailing7dayleases                    272243 non-null  Int64         
 63  trailing4weekaverageleases            272243 non-null  Int64         
 64  trailingadditional4weekaverageleases  264355 non-null  Int64         
 65  prioryeartrailing7dayleases           146805 non-null  Int64         
 66  rentablebedcnt                        231627 non-null  float64       
 67  sqftperbed                            224477 non-null  float64       
 68  occupiedbedcnt                        208630 non-null  float64       
dtypes: Int64(24), datetime64[ns](5), float64(33), object(7)
memory usage: 153.2+ MB
# Summary Statistics
summary_stats = df.describe()

# Display the collected information
print("Summary Statistics:\n", summary_stats)
Summary Statistics:
                                 date          academicyearbegindate  \
count                         278955                         278955   
mean   2023-08-28 04:18:44.155508480  2024-03-11 07:38:30.914663424   
min              2022-09-19 00:00:00            2023-09-18 00:00:00   
25%              2023-04-03 00:00:00            2023-09-18 00:00:00   
50%              2023-09-07 00:00:00            2023-09-18 00:00:00   
75%              2024-01-29 00:00:00            2024-09-16 00:00:00   
max              2024-06-12 00:00:00            2024-09-16 00:00:00   
std                              NaN                            NaN   

                 academicyearenddate                  weekbegindate  \
count                         278955                         278955   
mean   2025-03-12 16:33:29.201484032  2023-08-25 04:28:22.726245888   
min              2024-09-15 00:00:00            2022-09-19 00:00:00   
25%              2024-09-15 00:00:00            2023-04-03 00:00:00   
50%              2024-09-15 00:00:00            2023-09-04 00:00:00   
75%              2025-09-21 00:00:00            2024-01-29 00:00:00   
max              2025-09-21 00:00:00            2024-06-10 00:00:00   
std                              NaN                            NaN   

                         weekenddate  totalcommunitybeds  proximitytocampus  \
count                         278955            278955.0      278955.000000   
mean   2023-08-31 04:28:22.726245888          549.857576           0.960980   
min              2022-09-25 00:00:00                12.0           0.180000   
25%              2023-04-09 00:00:00               302.0           0.470000   
50%              2023-09-10 00:00:00               548.0           0.810000   
75%              2024-02-04 00:00:00               752.0           1.120000   
max              2024-06-16 00:00:00              1306.0           3.710000   
std                              NaN          297.242396           0.657985   

           vintage  newleasesustainablecap  renewalleasesustainablecap  ...  \
count     278955.0           269228.000000               210247.000000  ...   
mean   2007.786779               88.305756                   66.326602  ...   
min         1895.0                1.000000                    2.000000  ...   
25%         2005.0               15.000000                   16.000000  ...   
50%         2013.0               42.000000                   34.000000  ...   
75%         2018.0              116.000000                   96.000000  ...   
max         2025.0              629.000000                  526.000000  ...   
std      19.577296              108.196652                   74.725498  ...   

       trailingadditional4weekaverageapps  prioryeartrailing7dayapps  \
count                            264355.0                   146805.0   
mean                             1.854147                   2.479473   
min                                   0.0                        0.0   
25%                                   0.0                        0.0   
50%                                   0.0                        0.0   
75%                                   2.0                        3.0   
max                                 234.0                      184.0   
std                               4.16524                   5.724649   

       dailyleases  trailing7dayleases  trailing4weekaverageleases  \
count     272243.0            272243.0                    272243.0   
mean      0.271919             1.77746                    1.425994   
min            0.0                 0.0                         0.0   
25%            0.0                 0.0                         0.0   
50%            0.0                 0.0                         0.0   
75%            0.0                 2.0                         1.0   
max          415.0               415.0                       103.0   
std       1.476909            4.911513                    3.582247   

       trailingadditional4weekaverageleases  prioryeartrailing7dayleases  \
count                              264355.0                     146805.0   
mean                               1.380935                     1.928034   
min                                     0.0                          0.0   
25%                                     0.0                          0.0   
50%                                     0.0                          0.0   
75%                                     1.0                          2.0   
max                                   231.0                        180.0   
std                                3.619717                     4.929649   

       rentablebedcnt     sqftperbed  occupiedbedcnt  
count   231627.000000  224477.000000   208630.000000  
mean       140.205576     426.192071      130.244490  
min          1.000000      21.428571        1.000000  
25%         24.000000     334.834334       21.000000  
50%         64.000000     400.666667       60.000000  
75%        192.000000     492.000000      189.000000  
max       1664.000000    1700.000000     1528.000000  
std        168.482289     191.483701      155.171675  

[8 rows x 62 columns]
Exploratory Analysis to Determine Features
import pandas as pd
import numpy as np
from catboost import CatBoostRegressor, Pool
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns

# Drop records with null values
df.dropna(inplace=True)

# Feature Engineering: Extracting date and seasonal features
df['date'] = pd.to_datetime(df['date'])
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day'] = df['date'].dt.day

# Calculate week of the academic year
df['academicyearbegindate'] = pd.to_datetime(df['academicyearbegindate'])
df['week_of_academic_year'] = ((df['date'] - df['academicyearbegindate']).dt.days // 7) % 52

# Define Target Variable and Features
target = 'newweeklyleases'
exclude_cols = ['keystonereportingind', 'propertytype', 'cgmgmtstatus', 'propertycd', 
                'date', 'academicyearbegindate', 'academicyearenddate', 'weekbegindate', 'weekenddate', 'college', 'proximitytocampus', 'vintage']
features = df.drop(columns=[target] + exclude_cols)

X = df[features.columns]
y = df[target]

# Identify categorical features
categorical_cols = ['communityname', 'unittype']
cat_features_index = [X.columns.get_loc(col) for col in categorical_cols]

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create Pool for CatBoost
train_pool = Pool(X_train, y_train, cat_features=cat_features_index)
test_pool = Pool(X_test, cat_features=cat_features_index)

# Model Training
model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, verbose=100)
model.fit(train_pool)

# Model Prediction
y_pred = model.predict(test_pool)

# Model Evaluation
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

print(f"RMSE: {rmse}")

# Feature Importance
feature_importance = model.get_feature_importance(train_pool)
feature_importance_df = pd.DataFrame({'Feature': features.columns, 'Importance': feature_importance}).sort_values(by='Importance', ascending=False)
0:	learn: 18.4652983	total: 75.4ms	remaining: 1m 15s
100:	learn: 1.1875698	total: 3.45s	remaining: 30.7s
200:	learn: 0.7504065	total: 6.76s	remaining: 26.9s
300:	learn: 0.5429715	total: 10.1s	remaining: 23.5s
400:	learn: 0.4380110	total: 13.4s	remaining: 20s
500:	learn: 0.3774447	total: 16.8s	remaining: 16.7s
600:	learn: 0.3290485	total: 19.9s	remaining: 13.2s
700:	learn: 0.2945896	total: 23.1s	remaining: 9.87s
800:	learn: 0.2674184	total: 26.5s	remaining: 6.58s
900:	learn: 0.2448116	total: 29.7s	remaining: 3.26s
999:	learn: 0.2265434	total: 32.9s	remaining: 0us
RMSE: 0.48990956188582596
# Display Feature Importance
print("Feature Importance:")
print(feature_importance_df)
Feature Importance:
                                 Feature  Importance
16                     totalweeklyleases   48.555761
18                   newweeklysignedrent   13.842825
24                newtrailing60dayleases    7.031815
20                newtrailing30dayleases    5.895230
19              totaltrailing30dayleases    4.980554
49                    trailing7dayleases    4.902515
23              totaltrailing60dayleases    4.658571
3                 newleasesustainablecap    1.686465
59                 week_of_academic_year    1.523007
55                        occupiedbedcnt    1.107218
53                        rentablebedcnt    0.851006
32                     currentaskingrate    0.651847
22            newtrailing30daysignedrent    0.526825
17                 totalweeklysignedrent    0.509649
35             trailing4weekaverageleads    0.419970
4             renewalleasesustainablecap    0.393719
10                        ytdtotalleases    0.383088
7                      avgmarketunitrate    0.353247
50            trailing4weekaverageleases    0.302031
8                     marketunitprelease    0.165613
44                      trailing7dayapps    0.132620
54                            sqftperbed    0.131607
6                      avgmarketunitsqft    0.111260
0                     totalcommunitybeds    0.104546
5                    totalmarketunitbeds    0.089273
28                newyearlyleasevariance    0.085075
9                    marketunitoccupancy    0.079290
11                          ytdnewleases    0.056063
21          totaltrailing30daysignedrent    0.054209
40             trailing4weekaveragetours    0.041081
36   trailingadditional4weekaverageleads    0.039957
14          newleasesignedmonthlyrentusd    0.039612
46    trailingadditional4weekaverageapps    0.033023
15      renewalleasesignedmonthlyrentusd    0.026751
26            newtrailing60daysignedrent    0.023129
12                      ytdrenewalleases    0.021630
51  trailingadditional4weekaverageleases    0.019225
30           newyearlysignedrentvariance    0.018628
37            prioryeartrailing7dayleads    0.018420
41   trailingadditional4weekaveragetours    0.016294
45              trailing4weekaverageapps    0.016224
29         totalyearlysignedrentvariance    0.013032
31                weeklyaskingratechange    0.011869
47             prioryeartrailing7dayapps    0.010608
27              totalyearlyleasevariance    0.010079
52           prioryeartrailing7dayleases    0.009436
34                     trailing7dayleads    0.007287
13                  signedmonthlyrentusd    0.005943
25          totaltrailing60daysignedrent    0.005817
58                                   day    0.005135
2                               unittype    0.004168
57                                 month    0.003722
42            prioryeartrailing7daytours    0.003392
33                            dailyleads    0.002853
48                           dailyleases    0.002465
1                          communityname    0.002115
39                     trailing7daytours    0.002049
43                             dailyapps    0.000864
38                            dailytours    0.000292
56                                  year    0.000000
Feature Selection
# Select top N features based on importance
N = 20  # You can change this number based on your preference
top_features = feature_importance_df.head(N)['Feature'].tolist()

# Update the feature set
X_top = X[top_features]

# Train-Test Split with selected features
X_train_top, X_test_top, y_train, y_test = train_test_split(X_top, y, test_size=0.2, random_state=42)

# Create Pool for CatBoost with selected features
train_pool_top = Pool(X_train_top, y_train, cat_features=[X_top.columns.get_loc(col) for col in categorical_cols if col in top_features])
test_pool_top = Pool(X_test_top, cat_features=[X_top.columns.get_loc(col) for col in categorical_cols if col in top_features])

# Model Training
model_top = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, verbose=100)
model_top.fit(train_pool_top)

# Model Prediction
y_pred_top = model_top.predict(test_pool_top)

# Model Evaluation
mse_top = mean_squared_error(y_test, y_pred_top)
rmse_top = np.sqrt(mse_top)

print(f"RMSE after feature selection: {rmse_top}")
0:	learn: 18.4330755	total: 7.56ms	remaining: 7.55s
100:	learn: 1.2056986	total: 545ms	remaining: 4.85s
200:	learn: 0.7489124	total: 1.24s	remaining: 4.92s
300:	learn: 0.5615006	total: 2.03s	remaining: 4.71s
400:	learn: 0.4524395	total: 2.91s	remaining: 4.34s
500:	learn: 0.3856244	total: 3.75s	remaining: 3.73s
600:	learn: 0.3438858	total: 4.6s	remaining: 3.06s
700:	learn: 0.3073734	total: 5.5s	remaining: 2.35s
800:	learn: 0.2819480	total: 6.37s	remaining: 1.58s
900:	learn: 0.2581553	total: 7.24s	remaining: 795ms
999:	learn: 0.2414013	total: 8.18s	remaining: 0us
RMSE after feature selection: 0.48822623290253525
# Aggregate by academic year and week for more general trends
weekly_avg = df.groupby(['year', 'week_of_academic_year'])['newweeklyleases'].mean().reset_index()
plt.figure(figsize=(14, 7))
sns.lineplot(data=weekly_avg, x='week_of_academic_year', y='newweeklyleases', hue='year')
plt.title('Weekly Average New Leases Signed')
plt.show()
No description has been provided for this image
Hyperparameter Tuning
from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'iterations': [500, 1000],
    'depth': [4, 6, 8],
    'learning_rate': [0.01, 0.1],
    'l2_leaf_reg': [1, 3, 5, 7]
}

# Initialize CatBoost model
catboost_model = CatBoostRegressor(cat_features=cat_features_index, verbose=0)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=catboost_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, verbose=2)

# Fit GridSearchCV
grid_search.fit(X_train, y_train)

# Best parameters and best RMSE
best_params = grid_search.best_params_
best_rmse = np.sqrt(-grid_search.best_score_)

print(f"Best Parameters: {best_params}")
print(f"Best RMSE from Grid Search: {best_rmse}")
Fitting 3 folds for each of 48 candidates, totalling 144 fits
[CV] END depth=4, iterations=500, l2_leaf_reg=1, learning_rate=0.01; total time=   7.7s
[CV] END depth=4, iterations=500, l2_leaf_reg=1, learning_rate=0.01; total time=   7.7s
[CV] END depth=4, iterations=500, l2_leaf_reg=1, learning_rate=0.01; total time=   7.6s
[CV] END depth=4, iterations=500, l2_leaf_reg=1, learning_rate=0.1; total time=   7.8s
[CV] END depth=4, iterations=500, l2_leaf_reg=1, learning_rate=0.1; total time=   7.7s
[CV] END depth=4, iterations=500, l2_leaf_reg=1, learning_rate=0.1; total time=   7.8s
[CV] END depth=4, iterations=500, l2_leaf_reg=3, learning_rate=0.01; total time=   7.5s
[CV] END depth=4, iterations=500, l2_leaf_reg=3, learning_rate=0.01; total time=   7.3s
[CV] END depth=4, iterations=500, l2_leaf_reg=3, learning_rate=0.01; total time=   7.4s
[CV] END depth=4, iterations=500, l2_leaf_reg=3, learning_rate=0.1; total time=   7.6s
[CV] END depth=4, iterations=500, l2_leaf_reg=3, learning_rate=0.1; total time=   8.1s
[CV] END depth=4, iterations=500, l2_leaf_reg=3, learning_rate=0.1; total time=   7.6s
[CV] END depth=4, iterations=500, l2_leaf_reg=5, learning_rate=0.01; total time=   7.4s
[CV] END depth=4, iterations=500, l2_leaf_reg=5, learning_rate=0.01; total time=   7.5s
[CV] END depth=4, iterations=500, l2_leaf_reg=5, learning_rate=0.01; total time=   7.3s
[CV] END depth=4, iterations=500, l2_leaf_reg=5, learning_rate=0.1; total time=   7.4s
[CV] END depth=4, iterations=500, l2_leaf_reg=5, learning_rate=0.1; total time=   7.5s
[CV] END depth=4, iterations=500, l2_leaf_reg=5, learning_rate=0.1; total time=   7.3s
[CV] END depth=4, iterations=500, l2_leaf_reg=7, learning_rate=0.01; total time=   7.3s
[CV] END depth=4, iterations=500, l2_leaf_reg=7, learning_rate=0.01; total time=   7.4s
[CV] END depth=4, iterations=500, l2_leaf_reg=7, learning_rate=0.01; total time=   7.1s
[CV] END depth=4, iterations=500, l2_leaf_reg=7, learning_rate=0.1; total time=   7.4s
[CV] END depth=4, iterations=500, l2_leaf_reg=7, learning_rate=0.1; total time=   7.2s
[CV] END depth=4, iterations=500, l2_leaf_reg=7, learning_rate=0.1; total time=   7.2s
[CV] END depth=4, iterations=1000, l2_leaf_reg=1, learning_rate=0.01; total time=  14.9s
[CV] END depth=4, iterations=1000, l2_leaf_reg=1, learning_rate=0.01; total time=  14.8s
[CV] END depth=4, iterations=1000, l2_leaf_reg=1, learning_rate=0.01; total time=  14.4s
[CV] END depth=4, iterations=1000, l2_leaf_reg=1, learning_rate=0.1; total time=  14.9s
[CV] END depth=4, iterations=1000, l2_leaf_reg=1, learning_rate=0.1; total time=  14.7s
[CV] END depth=4, iterations=1000, l2_leaf_reg=1, learning_rate=0.1; total time=  15.1s
[CV] END depth=4, iterations=1000, l2_leaf_reg=3, learning_rate=0.01; total time=  14.0s
[CV] END depth=4, iterations=1000, l2_leaf_reg=3, learning_rate=0.01; total time=  14.3s
[CV] END depth=4, iterations=1000, l2_leaf_reg=3, learning_rate=0.01; total time=  14.1s
[CV] END depth=4, iterations=1000, l2_leaf_reg=3, learning_rate=0.1; total time=  15.0s
[CV] END depth=4, iterations=1000, l2_leaf_reg=3, learning_rate=0.1; total time=  15.0s
[CV] END depth=4, iterations=1000, l2_leaf_reg=3, learning_rate=0.1; total time=  14.8s
[CV] END depth=4, iterations=1000, l2_leaf_reg=5, learning_rate=0.01; total time=  13.8s
[CV] END depth=4, iterations=1000, l2_leaf_reg=5, learning_rate=0.01; total time=  14.4s
[CV] END depth=4, iterations=1000, l2_leaf_reg=5, learning_rate=0.01; total time=  14.4s
[CV] END depth=4, iterations=1000, l2_leaf_reg=5, learning_rate=0.1; total time=  14.8s
[CV] END depth=4, iterations=1000, l2_leaf_reg=5, learning_rate=0.1; total time=  15.1s
[CV] END depth=4, iterations=1000, l2_leaf_reg=5, learning_rate=0.1; total time=  14.9s
[CV] END depth=4, iterations=1000, l2_leaf_reg=7, learning_rate=0.01; total time=  13.7s
[CV] END depth=4, iterations=1000, l2_leaf_reg=7, learning_rate=0.01; total time=  14.3s
[CV] END depth=4, iterations=1000, l2_leaf_reg=7, learning_rate=0.01; total time=  13.8s
[CV] END depth=4, iterations=1000, l2_leaf_reg=7, learning_rate=0.1; total time=  14.7s
[CV] END depth=4, iterations=1000, l2_leaf_reg=7, learning_rate=0.1; total time=  14.4s
[CV] END depth=4, iterations=1000, l2_leaf_reg=7, learning_rate=0.1; total time=  14.5s
[CV] END depth=6, iterations=500, l2_leaf_reg=1, learning_rate=0.01; total time=  12.4s
[CV] END depth=6, iterations=500, l2_leaf_reg=1, learning_rate=0.01; total time=  12.5s
[CV] END depth=6, iterations=500, l2_leaf_reg=1, learning_rate=0.01; total time=  12.4s
[CV] END depth=6, iterations=500, l2_leaf_reg=1, learning_rate=0.1; total time=  12.6s
[CV] END depth=6, iterations=500, l2_leaf_reg=1, learning_rate=0.1; total time=  12.8s
[CV] END depth=6, iterations=500, l2_leaf_reg=1, learning_rate=0.1; total time=  13.0s
[CV] END depth=6, iterations=500, l2_leaf_reg=3, learning_rate=0.01; total time=  12.4s
[CV] END depth=6, iterations=500, l2_leaf_reg=3, learning_rate=0.01; total time=  12.1s
[CV] END depth=6, iterations=500, l2_leaf_reg=3, learning_rate=0.01; total time=  12.2s
[CV] END depth=6, iterations=500, l2_leaf_reg=3, learning_rate=0.1; total time=  12.9s
[CV] END depth=6, iterations=500, l2_leaf_reg=3, learning_rate=0.1; total time=  12.6s
[CV] END depth=6, iterations=500, l2_leaf_reg=3, learning_rate=0.1; total time=  13.1s
[CV] END depth=6, iterations=500, l2_leaf_reg=5, learning_rate=0.01; total time=  12.6s
[CV] END depth=6, iterations=500, l2_leaf_reg=5, learning_rate=0.01; total time=  12.1s
[CV] END depth=6, iterations=500, l2_leaf_reg=5, learning_rate=0.01; total time=  11.9s
[CV] END depth=6, iterations=500, l2_leaf_reg=5, learning_rate=0.1; total time=  12.3s
[CV] END depth=6, iterations=500, l2_leaf_reg=5, learning_rate=0.1; total time=  12.5s
[CV] END depth=6, iterations=500, l2_leaf_reg=5, learning_rate=0.1; total time=  12.2s
[CV] END depth=6, iterations=500, l2_leaf_reg=7, learning_rate=0.01; total time=  12.3s
[CV] END depth=6, iterations=500, l2_leaf_reg=7, learning_rate=0.01; total time=  12.3s
[CV] END depth=6, iterations=500, l2_leaf_reg=7, learning_rate=0.01; total time=  12.0s
[CV] END depth=6, iterations=500, l2_leaf_reg=7, learning_rate=0.1; total time=  12.3s
[CV] END depth=6, iterations=500, l2_leaf_reg=7, learning_rate=0.1; total time=  12.4s
[CV] END depth=6, iterations=500, l2_leaf_reg=7, learning_rate=0.1; total time=  12.2s
[CV] END depth=6, iterations=1000, l2_leaf_reg=1, learning_rate=0.01; total time=  24.0s
[CV] END depth=6, iterations=1000, l2_leaf_reg=1, learning_rate=0.01; total time=  25.1s
[CV] END depth=6, iterations=1000, l2_leaf_reg=1, learning_rate=0.01; total time=  25.0s
[CV] END depth=6, iterations=1000, l2_leaf_reg=1, learning_rate=0.1; total time=  25.4s
[CV] END depth=6, iterations=1000, l2_leaf_reg=1, learning_rate=0.1; total time=  25.5s
[CV] END depth=6, iterations=1000, l2_leaf_reg=1, learning_rate=0.1; total time=  25.3s
[CV] END depth=6, iterations=1000, l2_leaf_reg=3, learning_rate=0.01; total time=  24.2s
[CV] END depth=6, iterations=1000, l2_leaf_reg=3, learning_rate=0.01; total time=  24.5s
[CV] END depth=6, iterations=1000, l2_leaf_reg=3, learning_rate=0.01; total time=  24.0s
[CV] END depth=6, iterations=1000, l2_leaf_reg=3, learning_rate=0.1; total time=  24.8s
[CV] END depth=6, iterations=1000, l2_leaf_reg=3, learning_rate=0.1; total time=  24.7s
[CV] END depth=6, iterations=1000, l2_leaf_reg=3, learning_rate=0.1; total time=  24.8s
[CV] END depth=6, iterations=1000, l2_leaf_reg=5, learning_rate=0.01; total time=  23.9s
[CV] END depth=6, iterations=1000, l2_leaf_reg=5, learning_rate=0.01; total time=  23.9s
[CV] END depth=6, iterations=1000, l2_leaf_reg=5, learning_rate=0.01; total time=  24.1s
[CV] END depth=6, iterations=1000, l2_leaf_reg=5, learning_rate=0.1; total time=  24.5s
[CV] END depth=6, iterations=1000, l2_leaf_reg=5, learning_rate=0.1; total time=  25.4s
[CV] END depth=6, iterations=1000, l2_leaf_reg=5, learning_rate=0.1; total time=  24.7s
[CV] END depth=6, iterations=1000, l2_leaf_reg=7, learning_rate=0.01; total time=  23.7s
[CV] END depth=6, iterations=1000, l2_leaf_reg=7, learning_rate=0.01; total time=  23.4s
[CV] END depth=6, iterations=1000, l2_leaf_reg=7, learning_rate=0.01; total time=  23.8s
[CV] END depth=6, iterations=1000, l2_leaf_reg=7, learning_rate=0.1; total time=  24.7s
[CV] END depth=6, iterations=1000, l2_leaf_reg=7, learning_rate=0.1; total time=  24.2s
[CV] END depth=6, iterations=1000, l2_leaf_reg=7, learning_rate=0.1; total time=  24.6s
[CV] END depth=8, iterations=500, l2_leaf_reg=1, learning_rate=0.01; total time=  24.1s
[CV] END depth=8, iterations=500, l2_leaf_reg=1, learning_rate=0.01; total time=  24.3s
[CV] END depth=8, iterations=500, l2_leaf_reg=1, learning_rate=0.01; total time=  24.5s
[CV] END depth=8, iterations=500, l2_leaf_reg=1, learning_rate=0.1; total time=  25.1s
[CV] END depth=8, iterations=500, l2_leaf_reg=1, learning_rate=0.1; total time=  26.1s
[CV] END depth=8, iterations=500, l2_leaf_reg=1, learning_rate=0.1; total time=  25.0s
[CV] END depth=8, iterations=500, l2_leaf_reg=3, learning_rate=0.01; total time=  24.2s
[CV] END depth=8, iterations=500, l2_leaf_reg=3, learning_rate=0.01; total time=  24.7s
[CV] END depth=8, iterations=500, l2_leaf_reg=3, learning_rate=0.01; total time=  24.9s
[CV] END depth=8, iterations=500, l2_leaf_reg=3, learning_rate=0.1; total time=  25.5s
[CV] END depth=8, iterations=500, l2_leaf_reg=3, learning_rate=0.1; total time=  24.8s
[CV] END depth=8, iterations=500, l2_leaf_reg=3, learning_rate=0.1; total time=  24.6s
[CV] END depth=8, iterations=500, l2_leaf_reg=5, learning_rate=0.01; total time=  24.8s
[CV] END depth=8, iterations=500, l2_leaf_reg=5, learning_rate=0.01; total time=  24.9s
[CV] END depth=8, iterations=500, l2_leaf_reg=5, learning_rate=0.01; total time=  24.7s
[CV] END depth=8, iterations=500, l2_leaf_reg=5, learning_rate=0.1; total time=  24.7s
[CV] END depth=8, iterations=500, l2_leaf_reg=5, learning_rate=0.1; total time=  24.9s
[CV] END depth=8, iterations=500, l2_leaf_reg=5, learning_rate=0.1; total time=  24.7s
[CV] END depth=8, iterations=500, l2_leaf_reg=7, learning_rate=0.01; total time=  24.4s
[CV] END depth=8, iterations=500, l2_leaf_reg=7, learning_rate=0.01; total time=  24.1s
[CV] END depth=8, iterations=500, l2_leaf_reg=7, learning_rate=0.01; total time=  24.1s
[CV] END depth=8, iterations=500, l2_leaf_reg=7, learning_rate=0.1; total time=  24.6s
[CV] END depth=8, iterations=500, l2_leaf_reg=7, learning_rate=0.1; total time=  24.2s
[CV] END depth=8, iterations=500, l2_leaf_reg=7, learning_rate=0.1; total time=  24.4s
[CV] END depth=8, iterations=1000, l2_leaf_reg=1, learning_rate=0.01; total time=  49.2s
[CV] END depth=8, iterations=1000, l2_leaf_reg=1, learning_rate=0.01; total time=  49.5s
[CV] END depth=8, iterations=1000, l2_leaf_reg=1, learning_rate=0.01; total time=  50.0s
[CV] END depth=8, iterations=1000, l2_leaf_reg=1, learning_rate=0.1; total time=  50.7s
[CV] END depth=8, iterations=1000, l2_leaf_reg=1, learning_rate=0.1; total time=  50.3s
[CV] END depth=8, iterations=1000, l2_leaf_reg=1, learning_rate=0.1; total time=  50.7s
[CV] END depth=8, iterations=1000, l2_leaf_reg=3, learning_rate=0.01; total time=  48.3s
[CV] END depth=8, iterations=1000, l2_leaf_reg=3, learning_rate=0.01; total time=  48.6s
[CV] END depth=8, iterations=1000, l2_leaf_reg=3, learning_rate=0.01; total time=  48.4s
[CV] END depth=8, iterations=1000, l2_leaf_reg=3, learning_rate=0.1; total time=  49.1s
[CV] END depth=8, iterations=1000, l2_leaf_reg=3, learning_rate=0.1; total time=  50.3s
[CV] END depth=8, iterations=1000, l2_leaf_reg=3, learning_rate=0.1; total time=  50.2s
[CV] END depth=8, iterations=1000, l2_leaf_reg=5, learning_rate=0.01; total time=  48.8s
[CV] END depth=8, iterations=1000, l2_leaf_reg=5, learning_rate=0.01; total time=  48.1s
[CV] END depth=8, iterations=1000, l2_leaf_reg=5, learning_rate=0.01; total time=  47.5s
[CV] END depth=8, iterations=1000, l2_leaf_reg=5, learning_rate=0.1; total time=  49.1s
[CV] END depth=8, iterations=1000, l2_leaf_reg=5, learning_rate=0.1; total time=  50.0s
[CV] END depth=8, iterations=1000, l2_leaf_reg=5, learning_rate=0.1; total time=  49.7s
[CV] END depth=8, iterations=1000, l2_leaf_reg=7, learning_rate=0.01; total time=  48.1s
[CV] END depth=8, iterations=1000, l2_leaf_reg=7, learning_rate=0.01; total time=  48.3s
[CV] END depth=8, iterations=1000, l2_leaf_reg=7, learning_rate=0.01; total time=  46.4s
[CV] END depth=8, iterations=1000, l2_leaf_reg=7, learning_rate=0.1; total time=  48.7s
[CV] END depth=8, iterations=1000, l2_leaf_reg=7, learning_rate=0.1; total time=  48.6s
[CV] END depth=8, iterations=1000, l2_leaf_reg=7, learning_rate=0.1; total time=  48.2s
Best Parameters: {'depth': 6, 'iterations': 1000, 'l2_leaf_reg': 5, 'learning_rate': 0.1}
Best RMSE from Grid Search: 0.8883609035028858
Cross-Validation
from sklearn.model_selection import cross_val_score
from sklearn.base import BaseEstimator, RegressorMixin
from catboost import CatBoostRegressor, Pool

# Wrapper to handle Pool with categorical features in cross-validation
class CatBoostCVWrapper(BaseEstimator, RegressorMixin):
    def __init__(self, iterations=1000, learning_rate=0.1, depth=6, cat_features=None):
        self.iterations = iterations
        self.learning_rate = learning_rate
        self.depth = depth
        self.cat_features = cat_features
        self.model = None

    def fit(self, X, y):
        self.model = CatBoostRegressor(iterations=self.iterations, learning_rate=self.learning_rate, depth=self.depth, verbose=100)
        self.model.fit(Pool(X, y, cat_features=self.cat_features))
        return self

    def predict(self, X):
        return self.model.predict(Pool(X, cat_features=self.cat_features))

# Define the model with categorical feature indices
model_cv = CatBoostCVWrapper(iterations=1000, learning_rate=0.1, depth=6, cat_features=cat_features_index)

# Perform cross-validation
cv_scores = cross_val_score(model_cv, X, y, cv=5, scoring='neg_mean_squared_error')
cv_rmse_scores = np.sqrt(-cv_scores)

print(f"Cross-Validation RMSE Scores: {cv_rmse_scores}")
print(f"Mean CV RMSE: {cv_rmse_scores.mean()}")
0:	learn: 20.5320846	total: 20.6ms	remaining: 20.6s
100:	learn: 1.1288887	total: 3.5s	remaining: 31.2s
200:	learn: 0.7133007	total: 6.88s	remaining: 27.4s
300:	learn: 0.5302003	total: 10.3s	remaining: 23.9s
400:	learn: 0.4197923	total: 13.8s	remaining: 20.6s
500:	learn: 0.3559283	total: 17.2s	remaining: 17.1s
600:	learn: 0.3143506	total: 20.6s	remaining: 13.7s
700:	learn: 0.2821726	total: 24.3s	remaining: 10.4s
800:	learn: 0.2536346	total: 27.8s	remaining: 6.9s
900:	learn: 0.2327725	total: 31.2s	remaining: 3.42s
999:	learn: 0.2152516	total: 34.5s	remaining: 0us
0:	learn: 20.5491011	total: 24.7ms	remaining: 24.7s
100:	learn: 1.1808037	total: 3.56s	remaining: 31.7s
200:	learn: 0.7000402	total: 7.14s	remaining: 28.4s
300:	learn: 0.5193651	total: 10.6s	remaining: 24.6s
400:	learn: 0.4196776	total: 13.6s	remaining: 20.2s
500:	learn: 0.3601903	total: 17s	remaining: 16.9s
600:	learn: 0.3150541	total: 20.6s	remaining: 13.7s
700:	learn: 0.2835302	total: 23.9s	remaining: 10.2s
800:	learn: 0.2578381	total: 27.5s	remaining: 6.84s
900:	learn: 0.2352928	total: 30.7s	remaining: 3.37s
999:	learn: 0.2175673	total: 34s	remaining: 0us
0:	learn: 20.2256667	total: 21.6ms	remaining: 21.6s
100:	learn: 1.1346106	total: 3.37s	remaining: 30s
200:	learn: 0.6756391	total: 6.88s	remaining: 27.4s
300:	learn: 0.4989437	total: 10.3s	remaining: 23.8s
400:	learn: 0.4008738	total: 13.6s	remaining: 20.4s
500:	learn: 0.3425979	total: 16.9s	remaining: 16.8s
600:	learn: 0.3011864	total: 20.2s	remaining: 13.4s
700:	learn: 0.2705676	total: 23.5s	remaining: 10s
800:	learn: 0.2471832	total: 27s	remaining: 6.7s
900:	learn: 0.2270693	total: 30.5s	remaining: 3.35s
999:	learn: 0.2120658	total: 33.8s	remaining: 0us
0:	learn: 20.4735808	total: 47ms	remaining: 47s
100:	learn: 1.1100001	total: 3.19s	remaining: 28.4s
200:	learn: 0.6532651	total: 6.23s	remaining: 24.8s
300:	learn: 0.4934962	total: 9.49s	remaining: 22s
400:	learn: 0.3979974	total: 12.6s	remaining: 18.9s
500:	learn: 0.3424351	total: 15.9s	remaining: 15.9s
600:	learn: 0.3000683	total: 19.1s	remaining: 12.7s
700:	learn: 0.2689528	total: 22.5s	remaining: 9.61s
800:	learn: 0.2441138	total: 25.9s	remaining: 6.43s
900:	learn: 0.2237167	total: 29.3s	remaining: 3.22s
999:	learn: 0.2064764	total: 32.7s	remaining: 0us
0:	learn: 5.9605697	total: 21.9ms	remaining: 21.9s
100:	learn: 0.6393759	total: 3.14s	remaining: 27.9s
200:	learn: 0.3963400	total: 6.19s	remaining: 24.6s
300:	learn: 0.3016467	total: 9.2s	remaining: 21.4s
400:	learn: 0.2481103	total: 12.9s	remaining: 19.2s
500:	learn: 0.2128269	total: 16.3s	remaining: 16.2s
600:	learn: 0.1877791	total: 19.7s	remaining: 13.1s
700:	learn: 0.1695246	total: 23.1s	remaining: 9.84s
800:	learn: 0.1562011	total: 26.5s	remaining: 6.58s
900:	learn: 0.1445016	total: 30s	remaining: 3.29s
999:	learn: 0.1336359	total: 33.3s	remaining: 0us
Cross-Validation RMSE Scores: [ 1.22770632  0.64386915  3.07543097  1.22556823 28.70532892]
Mean CV RMSE: 6.975580716634889
Ensemble Methods - Combining multiple models
# Ensure categorical features are strings
categorical_cols = ['communityname', 'unittype']
df[categorical_cols] = df[categorical_cols].astype(str)

# Ensure no numerical values or NaNs in categorical columns
for col in categorical_cols:
    df[col] = df[col].fillna('Unknown').astype(str)
    df[col] = df[col].apply(lambda x: str(x) if not isinstance(x, str) else x)

# Check the categorical columns
def check_categorical_columns(df, categorical_cols):
    for col in categorical_cols:
        print(f"Column: {col}")
        print(f"Type: {df[col].dtype}")
        print(f"Unique Values: {df[col].unique()[:50]}")  # Display the first 10 unique values for brevity
        print("-" * 40)

check_categorical_columns(df, categorical_cols)
Column: communityname
Type: object
Unique Values: ['12 North' 'Arena District' 'Canvas Townhomes Morgantown'
 'Clear Creek Village' 'College Campanile Apartments'
 'Continuum Apartments' 'Copper Beech at Ames' 'Douglas Heights'
 'Ferry Street Flats' 'Fifty Twenty-Five' 'Fremont Station' 'Hill Place'
 'Landmark' 'Logan Square' 'M@College' 'Paloma University City'
 'Parkway Plaza' 'Rev' 'Revelry Flats | Milledgeville'
 'Ridge At Clear Creek' 'Seven07' 'Sol'
 'State College Collective on Northbrook' 'Tablerock'
 'The Cottages At Lake Tamaha' 'The Grove at Ames' 'The Grove at Auburn'
 'The Grove at Pullman' 'The Patterson Social' 'The Reserve Carrollton'
 'The Union' 'The Wilde' 'Towers On State' 'Varsity House Gainesville'
 'West Woods' 'Westmar Student Lofts' 'Arcadia'
 'Aspen By Traverse Commons' 'Canvas Townhomes Allendale'
 'Canvas Townhomes Columbia' 'Central House Columbia'
 'Central House On Stadium' 'Cerca Student Housing' 'College Town'
 'College Vue' 'Copper Beech at Harrisonburg' 'Copper Beech at Radford'
 'Domain At Waco' 'Element Communities Downtown Columbia'
 'Elm By Traverse Commons']
----------------------------------------
Column: unittype
Type: object
Unique Values: ['0BR/1BA' '1BR/1BA' '2BR/2BA' '4BR/2BA' '3BR/3BA' '3BR/2BA' '4BR/4BA'
 '2BR/1BA' '5BR/5BA' '4BR/4.5BA' '2BR/2.5BA' '3BR/3.5BA' '3BR/1BA'
 '4BR/3.5BA' '1BR/2BA' '2BR/1.5BA' '2BR/3BA' '5BR/4BA' '4BR/3BA'
 '5BR/5.5BA']
----------------------------------------
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from catboost import CatBoostRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LinearRegression

# Ensure categorical features are strings
categorical_cols = ['communityname', 'unittype']
df[categorical_cols] = df[categorical_cols].astype(str)

# Ensure no numerical values or NaNs in categorical columns
for col in categorical_cols:
    df[col] = df[col].fillna('Unknown').astype(str)
    df[col] = df[col].apply(lambda x: str(x) if not isinstance(x, str) else x)

# Check the categorical columns
def check_categorical_columns(df, categorical_cols):
    for col in categorical_cols:
        print(f"Column: {col}")
        print(f"Type: {df[col].dtype}")
        print(f"Unique Values: {df[col].unique()[:10]}")  # Display the first 10 unique values for brevity
        print("-" * 40)

check_categorical_columns(df, categorical_cols)

# Separate feature sets
X = df.drop(columns=['newweeklyleases', 'keystonereportingind', 'propertytype', 'cgmgmtstatus', 'propertycd', 
                'date', 'academicyearbegindate', 'academicyearenddate', 'weekbegindate', 'weekenddate', 'college', 'proximitytocampus', 'vintage'])
y = df['newweeklyleases']

# Identify categorical feature indices for CatBoost
cat_features_index = [X.columns.get_loc(col) for col in categorical_cols]

# One-Hot Encode categorical features for RandomForest and XGBoost
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
X_encoded = encoder.fit_transform(X[categorical_cols])
X_other = X.drop(columns=categorical_cols)
X_rf_xgb = np.hstack((X_other, X_encoded))

# Train-Test Split for both feature sets
X_train_rf_xgb, X_test_rf_xgb, y_train, y_test = train_test_split(X_rf_xgb, y, test_size=0.2, random_state=42)
X_train_catboost, X_test_catboost, _, _ = train_test_split(X, y, test_size=0.2, random_state=42)

# Double-checking data types for CatBoost
print("CatBoost Data Types Check")
for col in categorical_cols:
    print(f"CatBoost Column: {col}, Type: {X_train_catboost[col].dtype}, Unique Values: {X_train_catboost[col].unique()[:10]}")

# Initialize models
catboost_model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, cat_features=cat_features_index, verbose=0)
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
xgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.1, max_depth=6, random_state=42)

# Train individual models
catboost_model.fit(X_train_catboost, y_train)
rf_model.fit(X_train_rf_xgb, y_train)
xgb_model.fit(X_train_rf_xgb, y_train)

# Generate predictions for the training set
train_preds_catboost = catboost_model.predict(X_train_catboost)
train_preds_rf = rf_model.predict(X_train_rf_xgb)
train_preds_xgb = xgb_model.predict(X_train_rf_xgb)

# Generate predictions for the test set
test_preds_catboost = catboost_model.predict(X_test_catboost)
test_preds_rf = rf_model.predict(X_test_rf_xgb)
test_preds_xgb = xgb_model.predict(X_test_rf_xgb)

# Stack predictions
train_stack = np.column_stack((train_preds_catboost, train_preds_rf, train_preds_xgb))
test_stack = np.column_stack((test_preds_catboost, test_preds_rf, test_preds_xgb))

# Train meta-learner
meta_learner = LinearRegression()
meta_learner.fit(train_stack, y_train)

# Predict with meta-learner
y_pred_ensemble = meta_learner.predict(test_stack)

# Model Evaluation
mse_ensemble = mean_squared_error(y_test, y_pred_ensemble)
rmse_ensemble = np.sqrt(mse_ensemble)

print(f"RMSE of Ensemble Model: {rmse_ensemble}")
Column: communityname
Type: object
Unique Values: ['12 North' 'Arena District' 'Canvas Townhomes Morgantown'
 'Clear Creek Village' 'College Campanile Apartments'
 'Continuum Apartments' 'Copper Beech at Ames' 'Douglas Heights'
 'Ferry Street Flats' 'Fifty Twenty-Five']
----------------------------------------
Column: unittype
Type: object
Unique Values: ['0BR/1BA' '1BR/1BA' '2BR/2BA' '4BR/2BA' '3BR/3BA' '3BR/2BA' '4BR/4BA'
 '2BR/1BA' '5BR/5BA' '4BR/4.5BA']
----------------------------------------
CatBoost Data Types Check
CatBoost Column: communityname, Type: object, Unique Values: ['Flats At The Oval' 'Parkway Plaza' 'Fremont Station'
 'Ridge At Clear Creek' 'Westmar Student Lofts' 'Zuma'
 'Central House Columbia' 'Seven07' 'The Degree' 'The Lux']
CatBoost Column: unittype, Type: object, Unique Values: ['0BR/1BA' '3BR/2BA' '4BR/4BA' '2BR/2BA' '4BR/2BA' '3BR/3BA' '5BR/4BA'
 '1BR/1BA' '5BR/5.5BA' '3BR/1BA']
RMSE of Ensemble Model: 0.6416777433923959
# Add predictions to the original dataframe
df_test = df.loc[X_test_catboost.index].copy()
df_test['predictedweeklynewleases'] = y_pred_ensemble

# Merge predictions back into the original dataframe
df_combined = df.copy()
df_combined.loc[df_test.index, 'predictedweeklynewleases'] = df_test['predictedweeklynewleases']

# Display the updated dataframe with predictions
df_combined.head()
date	academicyearbegindate	academicyearenddate	weekbegindate	weekenddate	keystonereportingind	propertytype	totalcommunitybeds	college	proximitytocampus	...	trailingadditional4weekaverageleases	prioryeartrailing7dayleases	rentablebedcnt	sqftperbed	occupiedbedcnt	year	month	day	week_of_academic_year	predictedweeklynewleases
1326	2024-06-12	2024-09-16	2025-09-21	2024-06-10	2024-06-16	true	Student	808	Texas A&M University	0.68	...	0	0	41.0	344.000000	41.0	2024	6	12	38	NaN
1327	2024-06-12	2024-09-16	2025-09-21	2024-06-10	2024-06-16	true	Student	808	Texas A&M University	0.68	...	0	0	9.0	638.000000	9.0	2024	6	12	38	NaN
1328	2024-06-12	2024-09-16	2025-09-21	2024-06-10	2024-06-16	true	Student	808	Texas A&M University	0.68	...	10	9	1016.0	350.044444	1015.0	2024	6	12	38	NaN
6480	2024-06-12	2024-09-16	2025-09-21	2024-06-10	2024-06-16	true	Student	244	University of Oregon	0.53	...	3	3	228.0	266.000000	204.0	2024	6	12	38	NaN
6489	2024-06-12	2024-09-16	2025-09-21	2024-06-10	2024-06-16	true	Student	920	West Virginia University	2.02	...	1	1	62.0	650.000000	56.0	2024	6	12	38	NaN
5 rows × 74 columns

## df_combined = df.sort_values(by=['date', 'communityname', 'unittype'], ascending=[False, True, True])

## summary_stats = df_combined.describe()

# Display the collected information
## print("Summary Statistics:\n", summary_stats)

## df_combined.head(20)
domo.write_dataframe(df_combined, 'WeeklyNewLeasePredictions')
